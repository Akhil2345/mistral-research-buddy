# ğŸš€ Local AI Research Assistant â€” Offline, Private & Smart ğŸ›¡ï¸ğŸ§‘â€ğŸ’»

Your personal offline AI research team powered by âœ¨ **LangChain** + ğŸ¦™ **Mistral via Ollama**.  
Run intelligent agents that analyze, search, summarize, and synthesize â€” all 100% locally.

> ğŸ Think of it like having your own **AI research lab** on your laptop â€” no API keys, no cloud, no limits.

---

## ğŸŒŸ Project Goals

ğŸ¯ Build a **Multi-Agent Local AI Research Assistant** with:

- ğŸ§© Framework: `LangChain`  
- ğŸ¦™ LLM: Local `Mistral` via Ollama  
- ğŸ¤– Agents:  
  ğŸ” `Analyzer` â†’ ğŸŒ `Searcher` â†’ âœ‚ï¸ `Summarizer` â†’ ğŸ§  `Synthesizer`

---

## ğŸ“‚ Folder Structure

```bash
local_ai_searcher/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ analyzer.py
â”‚   â”œâ”€â”€ searcher.py
â”‚   â”œâ”€â”€ summarizer.py
â”‚   â””â”€â”€ synthesizer.py
â”œâ”€â”€ main.py
â”œâ”€â”€ llm.py
â””â”€â”€ requirements.txt


# ğŸ§ª Step 1: Clone the Repo
git clone https://github.com/yourusername/local_ai_searcher.git
cd local_ai_searcher

# ğŸ Step 2: Create Virtual Environment & Install Dependencies
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# ğŸ“¦ Step 3: Manual Install (If needed)
pip install langchain langchain-community

# ğŸ¦™ Step 4: Start Mistral LLM Locally
ollama run mistral
