# ğŸ§  Local AI Research Assistant â€“ Your Offline Research Copilot

**Private. Multi-Agent. LangChainâ€‘Powered.**  
A fully local system that breaks down complex topics into clean, summarized research using Mistral + LangChain â€” all running on your machine, no API needed.

> ğŸ“š Built for devs, researchers & students who want ChatGPTâ€‘style research â€” fully offline and customizable.

---

## âœ¨ What It Does

âœ… Accepts a research topic or technical question  
âœ… Breaks it down into multiple sub-queries  
âœ… Searches local files or online (optional)  
âœ… Summarizes and trims noisy content  
âœ… Synthesizes a final, simplified response

---

## ğŸ” Example Workflow

1. ğŸ’¬ You enter a topic: â€œHow do ZK-SNARKs work?â€  
2. ğŸ§  Analyzer breaks it into 3â€“5 sub-topics  
3. ğŸ” Searcher fetches local or online content  
4. âœ‚ï¸ Summarizer condenses each one  
5. ğŸ§ª Synthesizer writes the final output

---

## ğŸ§± Tech Stack

- ğŸ§© **LangChain**  
- ğŸ¦™ **Mistral LLM** via [Ollama](https://ollama.com)  
- ğŸ **Python 3.10+**

---

## âš™ï¸ Setup

```bash
# 1. Clone the repo
git clone https://github.com/yourusername/local_ai_searcher.git
cd local_ai_searcher

# 2. Create & activate environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start Mistral via Ollama
ollama run mistral

# 5. Run the Research Assistant
python main.py


